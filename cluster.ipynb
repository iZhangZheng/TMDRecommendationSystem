{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Traceback (most recent call last):\n",
       "  File \"/var/folders/g4/bh0x216x6_j83zjyf806bjn40000gn/T/kernel-PySpark-9959afa3-6927-47bb-9c5f-d6edb14ca888/pyspark_runner.py\", line 180, in <module>\n",
       "    eval(compiled_code)\n",
       "  File \"<string>\", line 5, in <module>\n",
       "  File \"/usr/local/Cellar/apache-spark/2.2.1/libexec/python/pyspark/context.py\", line 115, in __init__\n",
       "    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n",
       "  File \"/usr/local/Cellar/apache-spark/2.2.1/libexec/python/pyspark/context.py\", line 299, in _ensure_initialized\n",
       "    callsite.function, callsite.file, callsite.linenum))\n",
       "ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=IBM Spark Kernel, master=local[*]) created by __init__ at /var/folders/g4/bh0x216x6_j83zjyf806bjn40000gn/T/kernel-PySpark-9959afa3-6927-47bb-9c5f-d6edb14ca888/pyspark_runner.py:131 \n",
       "\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:158)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:158)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:157)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:280)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import array\n",
    "from math import sqrt\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "sc = SparkContext(appName=\"clusteringExample\")\n",
    "# Load and parse the data\n",
    "data = sc.textFile(\"/Users/apple/Desktop/5003-project/moviedata/TMD\")\n",
    "parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(parsedData, 2, maxIterations=10, initializationMode=\"random\")\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
